---
title: 'Take-Home Exercises'
date: 2025-10-31
description: "How to design effective take-home exercises that assess both ability and interest, with practical guidelines on scope, payment, and AI-assisted work"
tags: ['hiring', 'engineering-management']
---

import Callout from '@/components/Callout.astro'

Take-home exercises are one of the most valuable tools in a hiring manager's toolkit. However, you need to do them right; you need to be true to your reasons for why you do them, be clear about what you are trying to assess, and be open with your expectations. I want to share how I think about them.

The key thing I always conveyed to the candidates is that **the goal is to assess both their ability and interest**. The ability part is easy (or easier); we need to know if they can solve basic tech problems. The interest part is more subtle but equally important.

So, here's how I built take-home exercises at my various jobs. There are 3 important things in a good take-home exercise:

1. Make sure they are aligned with your company's values.
2. Make sure the scope is limited
3. Follow the process

## 1. Value Alignment

The goal of a take-home exercise is not to just see if the candidate can code, but see what the output looks and feels like. If fit and finish is important to you, you should let the candidate know in advance _and_ use that as a criterion.

This matters more than you think. One of our take-home challenges was primarily a data munging exercise. One candidate took the time not just to work with the data but also build a visualization. We didn't ask them to do it, but they wanted to. It showed not only initiative, but also the visualization was so tastefully done that we knew we could rely on their taste.

### Creating Role-Specific Exercises

Goes without saying, but you want to create very specific exercises for each role you are hiring. It's likely you won't get it right the first time and will do some edits, but here are some other guidelines:

1. Specific is better than vague. You should be able to define the exercise in one sentence.
2. The exercise should be reasonably fun—good candidates like interesting problems.
3. The goal is to see if they can code, not solve a hard CS problem (for most SaaS companies).
4. Make sure you have solved the problem yourself, but also be aware that there are multiple paths to god.

## 2. Limited Scope

My goal for all the challenges is that they can be solved "in an afternoon". For some candidates, this means a couple of hours, and for some others a few days. Either is fine, though the former is slightly better.

The temptation here is to make the exercise large. Resist it. A good exercise is limited in scope but has a lot of incidental complexity. In my experience, it's better to have a task that requires them to do one hard thing and then do the scaffolding around it as separate commits.

### Should You Pay People?

This also helps answer the "I want to be paid for this" question. I was surprised how this was almost never an issue; out of hundreds of candidates, only 2 people demanded payment. We rejected both, and one of them did the exercise anyway.

It's useful to give context on why I don't offer payment for this work. I openly told candidates the work would be representative of the work but it wouldn't be something we'd use ever (and it was clear from the exercise). We weren't trying to get anyone to do free labor.

I told candidates that we considered this part of our due diligence before moving to the next stage, which was either the on-site interview with the team or the work-trial. Either one is very expensive, especially for a small company. I found that candidates who are serious about a job search could fit in a couple hours in their week to help me assess them and gather resources on my part.

Again, you need to find your own wording here, but I was pleasantly surprised how many people immediately grokked it when I openly shared why we had take-home exercises in the first place. Honesty is always the best policy.

## 3. Follow the Process

This is the simplest part. I found it best to make a sharable Notion page that included an overview, detailed instructions, delivery instructions, FAQs, and the rest. Here's a very basic sample:

### Example

<Callout variant="example">
- Overview: We want you to build a simple "Carnival Spin Wheel" in JS.
- Detailed Instructions:
	- When the page loads, the user should see a colorful carnival spin wheel
	- They should be able to spin the wheel and win a prize. Each prize should have a dollar value.
	- There should be a counter that shows the total prize value earned.
	- The user should have 3 spins.
- Judging Criteria:
	- You'll be judged on how your wheel works as well as the code quality.
	- We use React and Typescript but you are welcome to use any client-side technology.
- Delivery Instructions
	- Please create a repo and share it with @cduruk
	- Make sure your repo has detailed running instructions for Mac.
</Callout>

We created a copy of this Notion page for each candidate and shared it with them only. I did this more for my own bookkeeping than some operational security concern about the questions leaking. One disgruntled candidate did share the post on a forum after we rejected them, and it hasn't been a huge problem.

I told people we operated on an honor code — we gave people 1 week to finish the exercise and to not spend more than an afternoon. I told them I'd check in after a week if I didn't hear back. This worked really well; interested candidates solved it in a few days and many people either never returned the exercise or kindly withdrew after I checked in.

## Assessment

This is generally the easy part, as long as you know what kind of person you are looking for. For me, the good candidates didn't just solve the problem but shared our values. We wanted folks who had good taste and wrote good code, where good was defined as "easy to change".

The assessment criteria here mirrors how I think about [measuring engineering productivity](/posts/measuring-engineering-productivity/) on the job—it's about output quality and alignment with values, not just raw metrics.

For front-end focused roles, this meant a working prototype that handles all the edge cases such as people actively messing with the UI, double clicks and such. For backend folks, the most valuable signal was always the simplicity of the code; fewer layers of indirection and followed common patterns in the language of their choice.

I don't claim to have all the answers here but some other criteria that seemed to work for me:

1. Simpler code almost always meant better outcomes.
2. Show your personality, especially for front-end exercises.
3. Great candidates often wrote tests, especially for backend take-homes.
4. Fewer dependencies meant better code.
5. Always test your own instructions (we did fix them occasionally but it always felt wrong)
6. There are diminishing returns; good candidates knew where to stop. See #1.
7. When in doubt, see [Zen of Python](https://peps.python.org/pep-0020/) 

## Vibe-Coded or AI-Assisted Work

It's clear that people will use AI to help write their code. I don't find it useful to guard against it. After all, it is a key part of everyone's toolkit these days and my goal is to be honest in my assessment of how people will get stuff done at work.

That being said, I think in the future hiring managers will have to pair more in person (or over Zoom) to go over these take-home projects. Until then, however, I've found a few things that work well.

Start by telling candidates that they should be very clear on how they programmed the exercise. I used to tell people that they can use third-party imports and now tell them that they can use AI. However, I do ask them to include what models they've used. This alone shows we care about this stuff.

Second, we try to find unique exercises that don't lend themselves to being "vibe coded". This is easier said than done. It's a good idea to give a few frontier models a try with your exercise to see what to expect.

Third, I tell them that our "in-person" section of the interview, should they advance to that stage, will involve further work on this project. This often "scares" people enough to make sure they don't blindly submit AI-coded projects.

I have considered asking for a commit history but based on how I use commits, I couldn't bring myself to use that as a criterion. It felt petty, given how different people use git differently in their personal flows, and also easy to game given the abilities of frontier models these days.

In the future, I want to experiment with giving people access to a model and to share their coding threads, like tools [Amp](https://www.youtube.com/watch?v=ar4IJNFmyIw) supports. I've seen this in action with folks like Mitchell Hashimoto [sharing entire workflows](https://ampcode.com/threads/T-24dca6a8-1a0a-4377-bfb2-5c4b77f8d3a9) and I really enjoyed it. Aside from being fun, this seems like a best of both worlds approach. I fully expect other tools like Claude Code to follow suit here.

Another idea I have is to actually share a couple vibe-coded examples as part of the instruction. This would help candidates understand what we are looking for, but also make it clear that we are aware these problems can be solved in many ways and we *have* looked at some AI-coded solutions.

## Final Thoughts

In Turkish we have a saying; you want people who are willing to "[put their hands under the rock](https://chatgpt.com/share/6904f54a-0dc4-800a-8267-1999de4c4c6a)" to lift it up, versus talking about doing the work. Those are the people you want to hire.People interview for tons of reasons; sometimes they are depressed and want to talk to someone. Sometimes they want to kick the tires to see if they can get a raise. And often they are looking for a new type of challenge, feel like they can do more than they are currently doing at their job. I wanted to hire those kinds of people and take-home exercises help with that. 

## More on Hiring

If you're curious about the broader pipeline—how many candidates you need to reach out to in order to make one hire—I built an [interactive hiring pipeline calculator](/posts/interactive-hiring-pipeline-calculator/) that might help you think through the numbers.

I've also [written before](/posts/job-interviews-farce/) about how chaotic and random hiring decisions are at the individual level. 

